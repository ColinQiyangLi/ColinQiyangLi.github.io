<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Q-learning with Adjoint Matching">
  <meta name="keywords" content="Reinforcement Learning, Action Chunking">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QAM</title>

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css"> 
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nunito" type='text/css'>
  <link rel="stylesheet" href="styles.css">
  <link rel="icon" href="https://example.com/path/to/favicon.ico">
</head>

<body>
<section class="hero">
  <div class="hero-body"  style="padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Q-learning with Adjoint Matching</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://colinqiyangli.github.io">Qiyang Li</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-family:Nunito">UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="http://arxiv.org/abs/2601.14234"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/colinqiyangli/qam"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 0%;">
      <picture style="width: 55%; max-width: 100%;" >
        <img alt="teaser figure" src="title-light.png">
      </picture>
      <picture style="width: 44%; max-width: 100%;" >
        <img alt="teaser figure" src="bar-light.png">
      </picture>
    </div>
  </div>
  <div class="column">
  <div class="container is-max-desktop has-text-centered">
    <div class="custom-green-box">
      <h2 class="title is-2" style="color: #66509B;"> TL;DR </h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              <strong>Q-learning with Adjoint Matching (QAM)</strong> applies <a href="https://arxiv.org/abs/2409.08861">Adjoint Matching</a> to policy optimization in Q-learning. 
              Compared to prior flow RL methods, QAM directly leverages the <b2>action gradient</b2> for policy optimization, 
              avoids potentially unstable <b2>backpropagation through time</b2>, 
              recovers the <b2>optimal behavior-constrained policy</b2> at the optimum, and 
              preserves the expressivity of a <b2>multi-step flow model</b2>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Background: What is Adjoint Matching?</h2>
    <div class="column">
      <div class="content has-text-justified">
        <p>
        Adjoint Matching <a href="https://arxiv.org/abs/2409.08861">(Domingo-Enrich et al., 2025)</a> is a recently proposed technique
        in the generative modeling literature that fine-tunes a <a href="https://arxiv.org/abs/2412.06264">flow-matching</a> 
        model to maximize a reward function Q under a standard KL regularization/constraint. Normally, generating/sampling a sample
        from a flow model involves sampling a noise and then solve an ordinary differential equation (ODE) via numerical integration. 
        One may also convert this ODE into 
        an SDE that admits the same path marginals subject to any noise schedule (Ïƒ), using the score function:
        </p> 
        <div class="custom-grey-box">
            <img src="equations/ode-sde.svg" style="width: 100%;" alt="\mathrm{d}X_t = f(X_t, t) \mathrm{d}t \quad \Leftrightarrow_d \quad  \mathrm{d}X_t = (f(X_t, t) - \sigma_t^2 \nabla_{X_t} \log p_t(X_t)) \mathrm{d}t + \sigma_t \mathrm{d} B_t">
        </div>
        <p>
        Since the optimal velocity field and the score has a simple relationship (e.g., see Eq. 4.79 in the <a href="https://arxiv.org/abs/2412.06264">flow-matching tutorial</a>), 
        we can simplify the ODE to SDE conversion further. Here, we assume the flow velocity field reconstructs the flow defined by <img src="equations/flow.svg" alt="X_t = tX_1 + (1-t)X_0">:
        </p>
        <div class="custom-grey-box">
            <img src="equations/ode-sde-v2.svg" style="width: 100%;" alt="\mathrm{d}X_t = f(X_t, t) \mathrm{d}t \quad \Leftrightarrow_d \quad  \mathrm{d}X_t = \left[\left(1+\frac{\sigma_t^2 t }{2(1-t)}\right)f(X_t, t) - \frac{\sigma_t^2 }{2(1-t)}X_t\right] + \sigma_t \mathrm{d} B_t"> 
        </div>
        <p>
        Under a memoryless noise schedule (e.g., <img src="equations/memoryless.svg" alt="\sigma_t = \sqrt{2(1-t)/t}">), the conversion can be further
        simplified into
        </p>
        <div class="custom-grey-box">
            <img src="equations/ode-sde-v3.svg" style="width: 100%;" alt="\mathrm{d}X_t = f(X_t, t) \mathrm{d}t \quad \Leftrightarrow_d \quad  \mathrm{d}X_t = \left[2f(X_t, t) - X_t/t\right] + \sigma_t \mathrm{d} B_t"> 
        </div>
        <p>
        The memoryless schedule ensures the noise is independent from the output from the SDE: <img src="equations/memoryless-effect.svg" alt="p(X_1) = p(X_1 \mid X_0)">. 
        This allows <a href="https://arxiv.org/abs/2409.08861">Domingo-Enrich et al., 2025</a> to use the following stochastic optimal control objective to any flow model against a reward function.
        </p>
        <div class="custom-grey-box">
            <img src="equations/soc.svg" style="width: 100%;" alt="L_{\mathrm{SOC}}(\theta) = \mathbb{E}_{\mathbf{X} \sim \mathrm{SDE}(f_\theta)}\left[ \int_0^1 \left(\frac{2}{\sigma_t^2}\|f_\theta(X_t, t) - f_\beta(X_t, t)\|_2^2\right) \mathrm{d} t - Q(X_1)\right]"> 
        </div>
        <p>
        At the optimum, fine-tuned velocity field generates the optimal KL-constrained distribution (an expoential tilt of the base flow model):
        </p>
        <div class="custom-grey-box">
          <center>
          <img src="equations/opt.svg" style="width: 40%;" alt="p_{\theta^\star}\star(X_1) \propto p_\beta(X_1) e^{Q(X_1)}"> 
          </center>
        </div>
        <p>
        Direct optimization of the SOC objective above requires backpropagation through time (BPTT). 
        The process can be equivalently represented as an 
        ODE of the adjoint state (the gradient of the loss function 
        with respect to the noisy sample) in continuous time from t=1 to t=0.
        </p>
        <div class="custom-grey-box">
          <center>
          <img src="equations/bam.svg" style="width: 100%;" alt="\mathrm{d} g(\mathbf X, t)= -\left[\nabla_{X_t} \left[2 {f_\theta}(X_t, t) - X_t/t\right] g(\mathbf X, t) + \frac2{\sigma^2_t} \nabla_{X_t} \|{f_\theta}(X_t, t) - {f_\beta}(X_t, t)\|_2^2\right]\mathrm{d}t, \quad g(\mathbf X, 1) = -\nabla_{X_1} Q(X_1)"> 
          </center>
        </div>
        <p>
        Notice how the ODE depends on the fine-tuned velocity field. 
        This means that any ill-conditioness in the fine-tuned flow model
        may blow up the ODE and making the optimization unstable. 
        Adjoint matching takes a step further by observing that 
        many terms in the adjoint state does not have effect on the optimal solution.
        This allows us to construct a "lean" adjoint state instead:
        </p>
        <div class="custom-grey-box">
          <center>
          <img src="equations/am.svg" style="width: 70%;" alt="\mathrm{d} \tilde g(\mathbf X, t)= -\nabla_{X_t} \left[2 {f_\beta}(X_t, t) - X_t/t\right] \tilde g(\mathbf X, t) , \quad \tilde g(\mathbf X, 1) = -\nabla_{X_1} Q(X_1)"> 
          </center>
        </div>
        <p>
        Now, the adjoint state ODE has no dependency on the fine-tuned velocity field.
        Instead, it only depends on the base velocity field. This means that
        the ill-conditioness in the fine-tuned flow model should not blow up adjoint state ODE, making
        the optimization more stable. The "lean" adjoint state has the same boundary condition (at t=1) 
        as the original adjoint state. With this "lean" adjoint state, we can complete the adjoint matching
        objective as follows:
        </p>
          <div class="custom-grey-box">
            <center>
              <img src="equations/am-loss.svg" style="width: 80%;" alt="L_{\mathrm{AM}}(\theta) = \mathbb{E}_{\mathbf{X} \sim \mathrm{SDE}(f_\theta)}\left[ \int_0^1 \left(\|2(f_\theta(X_t, t) - f_\beta(X_t, t))/\sigma_t + \sigma_t \tilde g(\mathbf X, t)\|_2^2\right) \mathrm{d} t \right]"> 
            </center>
          </div>
        <p>
        This is it! While we hope this provides a minimal summary for adjoint matching for our work, 
        the <a href="https://arxiv.org/abs/2409.08861">original paper</a> contains more detail 
        and provides a more general framework.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3" id="Results">Q-learning with Adjoint Matching</h2>
    <p>
    Expressive policies have been gaining popularity recently in RL (especially in offline RL) as they excel 
    in capturing multimodal distribution in prior data and synergize well with <a href="https://arxiv.org/abs/2304.13705">action chunking</a> 
    (e.g., <a href="https://arxiv.org/abs/2507.07969">QC</a> and <a href="https://arxiv.org/abs/2512.10926">DQC</a>). 
    A natural policy optimization objective is to find a policy that maximizes some value function (e.g., Q function) 
    while staying close to the behavior distribution in the prior data:
    </p>
    <div class="custom-grey-box">
      <center>
        <img src="equations/ebm.svg" style="width: 80%;" alt="{\arg\max}_\pi \mathbb{E}_{a \sim \pi(\cdot \mid s)}[Q(s, a)] \quad \mathrm{s.t.} \quad D_{\mathrm{KL}}(\pi \mid\mid \pi_\beta) \leq \epsilon(s)"> 
      </center>
    </div>
    <p>
    This objective has a closed-form solution of
    </p>
    <div class="custom-grey-box">
      <center>
        <img src="equations/ebm-close.svg" style="width: 50%;" alt="\pi^\star(a \mid s) \propto \pi_\beta(a \mid s) e^{\tau(s) Q_\phi(s, a)}"> 
      </center>
    </div>
    <p>
    Finding this optimal <strong>behavior-constrained policy</strong> is non-trivial and there has been many diffusion/flow RL methods
    proposed in recent years to tackle this. However, these methods often have their own failure modes. For example, 
    many existing methods akin to <a href="https://arxiv.org/abs/2105.05233">classifier</a>/<a href="https://arxiv.org/abs/2207.12598">classifier-free</a> 
    guidance approach which combines the score/velocity field of the base policy and the gradient of the value function additively during sampling. While this serves as 
    a reasonable approximation, it does not guarantee the action samples to recover the action distribution of the optimal behavior-constrained policy. 
    Some uses weighted behavior cloning (e.g., FAWAC from <a href="https://arxiv.org/abs/2502.02538">here</a>) where its optimum coincides 
    with the optimal behavior-constrained policy, but such approach has been found to be practically ineffective as it
    does not leverage the first-order information (i.e., action gradient) of the value function. 
    Other non-ideal characteristics include the need for backpropagation through time (through the diffusion/flow) 
    and distillation (e.g., <a href="https://arxiv.org/abs/2502.02538">FQL</a>) which can sacrifice the policy expressivity. 
    <br><br>
    In our work, we leverage adjoint matching, which exactly solves the same KL-constrained optimization problem, and more importantly 
    avoids all the aforementioned non-ideal characteristics in prior methods (see below).
    </p>
    <br>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0 auto; margin-bottom: 0px;">
      <figure>  
        <img src="conceptual.jpeg" style="width: 100%;" alt="Conceptual differences between QAM and prior flow RL methods.">
        <figcaption><b3>QAM brings all the advantageous characteristics in prior methods together.</b3></figcaption>
      </figure>
    </div>
    <br>
    <p>
    The result of it is <strong>QAM</strong>, a Q-learning algorithm that brings the advantages of prior methods together. It uses the action gradient from the value function directly to 
    fine-tune a flow policy towards the optimal behavior-constrained policy
    without using unstable backpropagation through time or sacrificing policy expressivity. 
    In practice, QAM can be summarized in just a few lines of pseudocode, largely following the fine-tuning procedure in the original adjoint matching paper 
    (but adapted to policy optimization).
    </p>
    <br>
    <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0 auto; margin-bottom: 0px;">
      <figure>  
        <img src="qam-algo.jpeg" style="width: 100%;" alt="Pseudocode for QAM.">
        <figcaption><b3>Pseudocode for QAM. We adapt <a href="https://arxiv.org/abs/2409.08861">adjoint matching</a> to policy optimization in Q-learning.</b3></figcaption>
      </figure>
    </div>
    <br>
    <p>
    In practice, we also find that it is often beneficial (especially for online fine-tuning) to relax the KL constraint such that the fine-tuned policy
    can output actions that are close "geometrically" close to the behavior actions in the prior data but have 
    low probability under the behavior distribution. We experimented with two variants. The first variant trains a <a href="https://arxiv.org/abs/2502.02538">FQL</a> policy
    on top of QAM's fine-tuned policy. The second variant trains a residual policy (the edit policy from <a href="https://arxiv.org/abs/2507.07986">EXPO</a>) to refine
    the action samples from QAM's fine-tuned policy. These two variants are called <strong>QAM-F</strong> and <strong>QAM-E</strong> respectively.
    As we will show in the next section, these two variants can further improve upon QAM and 
    the QAM-E variant exhibits strong online fine-tuning performance.
    </p>
  </div>  
</section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3" id="Results">Results</h2>
    <p>
    We evaluated QAM on <a href="https://seohong.me/projects/ogbench/">OGBench</a>, 
    a popular offline RL benchmark with a diverse set of tasks ranging from locomotion to manipulation. 
    All three variants of QAM exhibit strong offline performance across the board except on some locomotion tasks (each number reports 
    the aggregated performance of 5 tasks in each domain).
    </p>
    <br>
    <div class="column">
      <div class="content has-text-justified">
        <figure>
          <img src="main-table.jpeg" style="width: 100%;" alt="Main table for offline RL">
        <figcaption><b3>QAM exhibits strong offline RL performance across the board (12 seeds).</b3></figcaption>
        </figure>
      </div>
    </div>
    <hr>
    <p>
    We also find the QAM-E variant is well-suited for online fine-tuning. On some of the hardest domains from the benchmark, 
    our method exhibits strong sample-efficiency and robustness across the board (each plot reports the 
    aggregated performance of 5 tasks in each domain).
    </p>
    <br>
    <div class="column">
      <div class="content has-text-justified">
          <figure>
          <img src="main-finetune.svg" style="width: 100%;" alt="Main plot for online fine-tuning">
            <figcaption><b3>The QAM-E variant of our approach exhbits strong and robust online fine-tuning capability (12 seeds).</b3></figcaption>
          </figure>
      </div>
    </div>
    <hr>
    <p>
    That is all for now! We have released our code (<b2>both our method and all baselines!!</b2>) as well as the <b2>experiment data</b2> for all our results at <a href="https://github.com/colinqiyangli/qam">github.com/colinqiyangli/qam</a>. 
    If you are interested in learning more about our work, come check out <a href="http://arxiv.org/abs/2601.14234">our paper on arXiv!</a>
    </p>
  </div>
     
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre>
<code>@article{li2026qam,
  author = {Qiyang Li and Sergey Levine},
  title  = {Q-learning with Adjoint Matching},
  conference = {arXiv Pre-print},
  year = {2026},
  url = {http://arxiv.org/abs/2601.14234},
}</code></pre></div>
</section>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>