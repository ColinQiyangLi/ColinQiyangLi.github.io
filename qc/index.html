<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Sample-Efficient Reinforcement Learning with Action Chunking">
  <meta name="keywords" content="Reinforcement Learning, Action Chunking, ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>QC</title>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css"> 
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nunito" type='text/css'>
  <link rel="stylesheet" href="styles.css">
  <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
  <link rel="icon" href="https://example.com/path/to/favicon.ico">
</head>

<body>

<section class="hero">
  <div class="hero-body"  style="padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Reinforcement Learning with Action Chunking</h1>
          <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://colinqiyangli.github.io">Qiyang Li</a>,
            </span>
            <span class="author-block">
              <a href="https://zhouzypaul.github.io/">Zhiyuan Zhou</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-family:Nunito">UC Berkeley</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2507.07969"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/colinqiyangli/qc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 1%;">
      <img src="teaser.png" alt="teaser figure" style="width: 49%; max-width: 100%;" />
      <img src="agg-ablation.svg" alt="teaser figure" style="width: 49%; max-width: 100%;" />
    </div>

    <div class="container is-max-desktop has-text-centered">
    <div class="custom-grey-box">
      <h2 class="title is-2" style="color: #CB297B;"> TL;DR </h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              <strong>Q-chunking</strong> runs RL on a temporally extended action (<b>action chunking</b>) space
              with an <b>expressive behavior constraint</b> to leverage prior data for 
              improved <strong>exploration</strong> and <strong>online sample efficiency</strong>.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>

    <div class="video-section" style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 1rem;">
      <div style="flex: 1 1 100%; max-width: 49%; text-align: center;">
        <h3 class="title is-2" style="color: #CB297B;">with Q-chunking</h3>
        <video width="100%" controls autoplay muted loop>
          <source src="h5.mp4" type="video/mp4" />
        </video>
      </div>

      <div style="flex: 1 1 100%; max-width: 49%; text-align: center;">
        <h3 class="title is-2" style="color: #0076ba;">without Q-chunking</h3>
        <video width="100%" controls autoplay muted loop>
          <source src="h1.mp4" type="video/mp4" />
        </video>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <div class="custom-box">
      <h2 class="title is-2"> Abstract </h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              We present <strong>Q-chunking</strong>, a simple yet effective recipe for improving reinforcement learning (RL) algorithms for 
              long-horizon, sparse-reward tasks. Our recipe is designed for the offline-to-online RL setting, where 
              the goal is to leverage an offline prior dataset to maximize the sample-efficiency of online learning. 
              Effective exploration and sample-efficient learning remain central challenges in this setting, 
              as it is not obvious how the offline data should be utilized to acquire a good exploratory policy. 
              Our key insight is that <strong>action chunking</strong>, a technique popularized in imitation learning where sequences 
              of future actions are predicted rather than a single action at each timestep, can be applied to 
              temporal difference (TD)-based RL methods to <strong>mitigate the exploration challenge</strong>. 
              Q-chunking adopts action chunking by directly running RL in a 'chunked' action space, 
              enabling the agent to (1) leverage <strong>temporally consistent behaviors</strong> from offline data for 
              more effective online exploration and (2) use <strong>unbiased n-step backups</strong> for more stable and 
              efficient TD learning. Our experimental results demonstrate that Q-chunking exhibits strong 
              offline performance and online sample efficiency, outperforming prior best offline-to-online 
              methods on a range of long-horizon, sparse-reward manipulation tasks. 
            </p>
          </div>
        </div>
      </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Q-chunking = Action Chunking RL + Behavior Constraint</h2>
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            Our first design principle is to run a standard TD-based actor-critic RL algorithm on <b>action sequences</b>. 
            That is, at every h time steps, the actor predicts a sequence of h actions and then execute them one-by-one in sequence.
            Both the critic and the actor are also trained via a standard TD loss but on this temporally extended action space. 
            This allows us to perform <strong>unbiased n-step return backups</strong>, improving <b>value learning stability</b> and <b>efficiency</b>.
            Our second design principle is to impose a good <b>behavior constraint</b> on the temporally extended action sequence. 
            Intuitively, this allows us to leverage temporally coherent action sequences in the offline dataset. 
            This is particularly advantageous in the temporally extended action space compared to in the original action space because offline 
            prior data (especially in the real world) may often exhibit <b>non-Markovian structure</b> 
            that cannot be well captured by a Markovian behavior constraint. As a result, our policy is able to generate 
            <strong>temporally coherent actions</strong> for more effective online exploration.
          </p>
          
        <div class="custom-grey-box">
            <h3>Standard TD-based RL</h3>

            <img src="normal.svg" style="width: 75%;" alt="RLPD-AC equations">
            <!-- </div>
            <div class="column"> -->
        </div>

        <div class="custom-blue-box">
          <h3>Q-chunking</h3>

          <img src="qc.svg" style="width: 100%;" alt="RLPD equations">
        </div>

        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">A Practical Q-chunking Algorithm: QC</h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p>
              We instantiate this idea as <strong>QC</strong>, a simple and effective offline-to-online RL algorithm. 
              We first train an expressive behavior cloning flow policy. Then, we implicitly represent a policy using samples from this behavior policy. 
              In particular, we use <b>Best-of-N (BFN)</b> sampling where we sample N actions and select the one that maximizes the Q function. 
              This is for both selecting actions during <b>online exploration</b> and <b>TD backup</b>. 
              QC can be summarized within only two lines: 
            </p>

            <div class="custom-blue2-box">
              <h3>Online exploration</h3>
              <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
                <img src="emaq.svg" style="width: 100%;" alt="EMaQ equations"> 
              </div>
              
              <h3>TD backup</h3>
              <div style="display: flex; justify-content: center; gap: 1%; width: 75%; margin: 0 auto; margin-bottom: 40px;">
                <img src="qc-backup.svg" style="width: 99%;" alt="RLPD-AC equations"> 
              </div>
            </div>
            <p>
              Best-of-N sampling is not a new idea in RL and has been previously explored in many prior works with different implementation details 
              (e.g., EMaQ (<a href="https://arxiv.org/abs/2007.11091">Ghasemipour et al., 2020</a>), 
              SfBC (<a href="https://arxiv.org/abs/2209.14548">Chen et al., 2022</a>)
              and DAC (<a href="https://arxiv.org/abs/2405.20555">Fang et al., 2025</a>)).
              However, using this idea in the context of online RL and action chunking is still under-explored. We found that 
              this incredibly simple implicit policy parameterization is quite effective at enforcing a behavior constraint while maximizing the value for an action chunking policy (see results below).
            </p> 
          </div>
        </div>
      </div>
    </section>

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3">Results</h2>
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
            <img src="main-all-ablation.svg" style="width: 100%;" alt="Main Results">
          </div>
          <p>
            The design principles of (1) running RL with action chunking, and 
            (2) imposing a behavior constraint on the 'chunked' actions is a general recipe that 
            synerize with existing offline-to-online RL algorithms very well because most of them are 
            already eqiupped with a behavior constraint! 
            
            This allows us to develop <strong>QC-FQL</strong>, another effective offline-to-online RL method that builds directly on top of 
            <b>FQL</b> (<a href="https://seohong.me/projects/fql/">Park et al., 2025</a>). 
            As what we will show below, <strong>QC-FQL</strong> also achieves significant improvements over <b>FQL</b>. 
            BFN-n and FQL-n are n-step return baselines for QC and QC-FQL respectively.
          </p>
          <div style="display: flex; justify-content: center; gap: 1%; width: 90%; margin: 0 auto; margin-bottom: 40px;">
            <img src="main-all.svg" style="width: 100%;" alt="Main Results">
          </div>
        </div>
      </div>
    </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre>
<code>@article{li2025qc,
  author = {Qiyang Li and Zhiyuan Zhou and Sergey Levine},
  title  = {Sample-Efficient Reinforcement Learning with Action Chunking},
  conference = {arXiv Pre-print},
  year = {2025},
  url = {http://arxiv.org/abs/2507.07969},
}</code></pre></div>
</section>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>