<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Decoupled Q-Chunking">
  <meta name="keywords" content="Reinforcement Learning, Action Chunking">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DQC</title>

  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet"> -->

  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-carousel@4.0.4/dist/css/bulma-carousel.min.css"> 
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma-slider@2.0.5/dist/css/bulma-slider.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Nunito" type='text/css'>
  <link rel="stylesheet" href="styles.css">
  <!-- <link rel="stylesheet" href="./static/css/index.css"> -->
  <link rel="icon" href="https://example.com/path/to/favicon.ico">
</head>

<body>
  <!-- <script src="https://cdn.jsdelivr.net/npm/darkmode-js@1.5.5/lib/darkmode-js.min.js"></script>
  <script>
  var options = {
    autoMatchOsTheme: false // default: true
  }

  const darkmode = new Darkmode(options);
  darkmode.showWidget();
  </script> -->

<section class="hero">
  <div class="hero-body"  style="padding-bottom: 0;">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Decoupled Q-Chunking</h1>
          <!-- <h2 class="title" style="font-size: large; color: grey;"></h2> -->
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://colinqiyangli.github.io">Qiyang Li</a>,
            </span>
            <span class="author-block">
              <a href="https://seohong.me/">Seohong Park</a>,
            </span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block" style="font-family:Nunito">UC Berkeley</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/xxxx.xxxxx"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/colinqiyangli/dqc"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="padding-top: 0;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="display: flex; flex-wrap: wrap; justify-content: space-between; gap: 0%;">
      <picture style="width: 48%; max-width: 100%;" >
        <img alt="teaser figure" src="teaser-tran-grad-small.png">
      </picture>
      <picture style="width: 48%; max-width: 100%;" >
        <img alt="teaser figure" src="dqc-bar-tran.png">
      </picture>
    </div>

    <div class="container is-max-desktop has-text-centered">
    <div class="custom-green-box">
      <h2 class="title is-2" style="color: #00A89D;"> TL;DR </h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              <strong>Decoupled Q-chunking</strong> improves upon <a href="https://colinqiyangli.github.io/qc/">Q-chunking</a> by decoupling the chunk size of the policy
              from that of the critic. <b>Policies</b> with <b>short action chunks</b> are easier to learn and 
              <b2>critics</b2> with <b2>long action chunks</b2> speed up value learning.
            </p>
          </div>
        </div>
      </div>
    </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop has-text-centered">
    <div class="custom-box">
      <h2 class="title is-2" id="Content"><b3>Table of Content</b3></h2>
      <div class="columns">
        <div class="column">
          <div class="content has-text-justified">
            <p class="is-size-5">
              <h3><b3>> Part A | Theory</b3> — Why decoupling?<a href="#Theory"> ➜</a> </h3>

              <h3><b3>> Part B | Practical Algorithm</b3> — DQC <a href="#Practical"> ➜</a> </h3>

              <h3><b3>> Results</b3> <a href="#Results"> ➜</a></h3>
            </p>
          </div>
        </div>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3" id="Theory">Part A: Why decoupling?  <a href="#Content">↺</a> </h2>
    <div class="columns">
      <div class="column">
        <div class="content has-text-justified">
          <p>
            In QC, the chunked critic is learned with n-step return backups, but there is a problem—the learned value function may not converge
             to the correct value due to a subtle discrepancy:
          </p>
          
        <div class="custom-purple-box">
            <h3>What QC should be doing..</h3>
            <img src="equations/qc-should.svg" style="width: 100%;" alt="\begin{align}
Q(s_t, a_{t:t+h}) \leftarrow \mathbb{E}_{\color[rgb]{0.7929,0.1602,0.4804} s_{t'+1} \sim T(\cdot \mid s_{t'}, a_{t'})}\left[\sum_{t'=t}^{t+h-1} \gamma^{t'-t}r(s_{t'}, a_{t'}) + \gamma^h Q(s_{t+h}, \pi(s_{t+h}))\right]
\end{align}">
        </div>

        <div class="custom-blue-box">
            <h3>What QC is actually doing..</h3>
            <img src="equations/qc-actual.svg" style="width: 100%;" alt="\begin{align}
Q(s_t, a_{t:t+h}) \leftarrow \mathbb{E}_{\color[rgb]{0,0.46,0.7265} s_{t+1:t+h+1}  \sim P_\mathcal{D}(\cdot \mid s_t, a_{t:t+h}) }\left[\sum_{t'=t}^{t+h-1} \gamma^{t'-t}r(s_{t'}, a_{t'}) + \gamma^h Q(s_{t+h}, \pi(s_{t+h}))\right]
\end{align}">
            <!-- <img src="qc.svg" style="width: 100%;" alt="RLPD equations"> -->
        </div>

        <p>
        Intuitively, as long as the data collection policy <b>reacts</b> to 
        the observation/state feedback in the middle of an action chunk, the 
        resulting state and reward distribution will deviate from that of executing
        the action chunk <b2>open-loop</b2>.
        To characterize this discrepancy formally, we introduce a notion of data consistency, 
        as formalized below:
        </p>

        <hr>

        <div class="custom-grey-box">
          <h3>Definition (informal): Open-loop consistency (OLC)</h3>
          <img src="equations/olc.svg" style="width: 100%;" alt="\begin{align}
D_{\mathrm{KL}}(\underbrace{T(s_{t+h'} \mid s_t, a_{t:t+h'})}_{\text{take $a_{t:t+h'}$ open-loop}} \mid\mid P_{\mathcal{D}}(s_{t+h'} \mid s_t, a_{t:t+h})) \leq \varepsilon_h, \forall h' \in \{1, 2, \cdots, h\}
\end{align}">
        </div>
        
        <p>
          <b3>OLC</b3> measures how much the 'open-loop' state-distribution can deviate from
          the 'reactive' state-distribution induced by the data collection policy. With this
          definition, we can formally show that QC yields a near-optimal action chunking policy
          when the data is open-loop consistent:
        </p>

        <hr>
        
        <div class="custom-green-box">
          <h3>Theorem (informal): QC learns a near-optimal action chunking policy under OLC.
          </h3>
          <br>
          <center>
          <img src="equations/thm-qc.svg" style="width: 65%;" alt="\begin{align}
\|V^\star - V_{\mathrm{QC}}\|_\infty \leq \Theta\left(\frac{\varepsilon_h}{(1-\gamma)(1-\gamma^h)}\right)
\end{align}">
          </center>
        </div>

        <p>
          This bound is tight, meaning that the worst-case QC sub-optimality scales linearly with 
          ε. When the data is not open-loop consistent, there exists an MDP where the action chunking policy 
          learned from the chunked critic is <b4>arbitrarily bad</b4>.
          In general, the degree of open-loop consistency (ε) is orthogonal to the
          sub-optimality of the data (which can hurt n-step return backup). So as long as the data distribution
          is sub-optimal while being open-loop consistent, critic chunking is preferred over n-step return backup:
        </p>
        
        <div class="custom-blue-box">
          <h3>Proposition (informal): QC is better than n-step return backup when</h3>
          <br>
          <center>
            <img src="equations/prop-compare.svg" style="width: 100%;" alt="\begin{align}
&\underbrace{Q^\star(s_t, a_t) -  \mathbb{E}_{P_{\mathcal{D}}(s_{t+1:t+h+1}, a_{t+1:t+h} \mid s_t, a_t)}\left[\sum_{t'=t}^{t+h-1} \gamma^{t'-t}r(s_{t'}, a_{t'})+ \gamma^h V^\star(s_{t+h})\right]}_{\text{sub-optimality of the data distribution}} > O\left(\frac{\varepsilon_h}{1-\gamma^h}\right), \\
&\text{where } Q^\star, V^\star \text{ are the value functions of the optimal policy $\pi^\star$}.
\end{align}">
          </center>
        </div>

        <p>
          Up to now, we have characterized the condition under which Q-chunking should be preferred over n-step return backup,
          but action chunking policy is still fundamentally <b4>limited in reactivity</b4> as it must execute all actions in each chunk <b2>open-loop</b2>.
          A common trick that robotists use for action chunking is to actually replan more frequently than the chunk size, where only a partial action chunked
          is executed. But can we still provide theoretical guarantees with this 'decoupling' setup? 
          The answer is yes, at least in the special case where only the first action is taken in the action chunk 
          (i.e., closed-loop execution of the learned action chunking policy).
        </p>

        <div class="custom-blue-box">
          <h3>Proposition (informal): closed-loop execution of QC policy is also near-optimal under OLC</h3>
          
          <center>
          <img src="equations/prop-dqc.svg" style="width: 65%;" alt="\begin{align}
\|V^\star - V_{\mathrm{DQC}}\|_\infty \leq O\left(\frac{\varepsilon_h}{(1-\gamma)^2(1-\gamma^h)}\right)
\end{align}">
          </center>
        </div>

        <p>
          The intuition here is simple—if the action chunking policy is near-optimal, then (perhaps not surprisingly) 
          the first action in each of the action chunk cannot be too sub-optimal. But this result is not as satisfying as the 
          QC result above, because in the worst case, the closed-loop execution performance can degrade up to a factor of 1/(1-γ) (the effective horizon).
        </p>
        <hr>
        <p>
          In <a href="https://arxiv.org/pdf/xxxx.xxxxx">our paper</a>, we also characterized a new set of conditions under 
          which such closed-loop execution is guaranteed to be close to the 
          optimal closed-loop policy even when the data is not open-loop consistent.
          This is where the decoupling of the execution length of the action chunk 
          from the critic chunking length truly shines.
        </p>

        <div class="custom-grey-box">
          <h3>Definition (informal): Bounded Optimality Variability (BOV)</h3>
          <br>
          <center>
          <img src="equations/bov.svg" style="width: 100%;" alt="\begin{align}
&\text{Let } \mathcal{D} \text{ be a mixture of } \{\mathcal{D}_1, \mathcal{D}_2, \cdots, \mathcal{D}_M\} \text{ with } \mathcal{D}^\star \text{ being one of } \mathcal{D}_i. \text{ Both of the following conditions hold:}\\
& \\
& \text{1. Local condition:} \max_{\color[rgb]{0,0.4804,0.4609}\mathrm{supp}( P_{\mathcal{D}^i}(\cdot \mid s_t, a_t))}\left[R_{t:t+h} +\gamma^h V^\star(s_{t+h})\right] - \min_{\color[rgb]{0,0.4804,0.4609}\mathrm{supp}(P_{\mathcal{D}^i}(\cdot \mid s_t, a_t))}\left[R_{t:t+h} +\gamma^h V^\star(s_{t+h})\right] \leq \vartheta^L_h, \forall i \in \{1, 2, \cdots, M\}, \\
& \text{2. Global condition:}  \max_{\color[rgb]{0,0.4804,0.4609}\mathrm{supp}(P_{\mathcal{D}}(\cdot \mid s_t, a_{t:t+h}))}\left[R_{t:t+h} +\gamma^h V^\star(s_{t+h})\right] - \min_{\color[rgb]{0,0.4804,0.4609}\mathrm{supp}(P_{\mathcal{D}}(\cdot \mid s_t, a_{t:t+h}))}\left[R_{t:t+h} +\gamma^h V^\star(s_{t+h})\right] \leq \vartheta^G_h.
\end{align}">
          </center>
        </div>

        <p>
          Intuitively, <b3>BOV</b3> assumes the data is a mixture of multiple sources 
          (e.g., expert data, scripted policy), where each source exhibits small variability in
          terms of the optimality of the h-step returns (local), and the overall variability (across mixture components) 
          <b4>conditioned on action chunk</b4> is also small. Under BOV, we can show that the closed-loop execution of the
          action chunking policy is guaranteed to be near-optimal, regardless of
          how open-loop <i>inconsistent</i> the data is.
        </p>

        <div class="custom-blue-box">
          <h3>Theorem (informal): DQC is near-optimal under BOV</h3>
          <br>
          <center>
          <img src="equations/thm-dqc.svg" style="width: 65%;" alt="\begin{align}
\|V^\star - V_{\mathrm{DQC}}\| \leq \Theta\left( \frac{\vartheta^L_h}{1-\gamma} + \frac{\vartheta^G_h + \gamma^h\min(\vartheta^L_h,\vartheta^G_h)}{(1-\gamma)(1-\gamma^h)}\right)
\end{align}">
          </center>
        </div>

        <!-- <p>
          Now, it has become clear that the closed-loop execution of the action chunking policy (DQC)
          enjoys theoretical guarantees under both OLC and 
          BOV condition whreas QC is only provably near-optimal under OLC. 
        </p> -->
        <hr>
        <p>
          <b>In summary</b>, when the data is OLC and sub-optimal, 
          QC is preferred over n-step return backup. The closed-loop execution of the action chunking policy (DQC)
          enjoys theoretical guarantees under both OLC and 
          BOV condition whreas QC is only provably near-optimal under OLC, making DQC seem to be a more robust
          choice conceptually. 
          <!-- When the data is not strongly open-loop consistent, 
          QC can perform arbitrily bad while the closed-loop execution of the learned action chunking policy 
          is still guaranteed to be near-optimal under the bounded optimality variability condition.  -->
          This inspired us to develop a practical algorithm to effective leverage this, which we will describe next.
        </p>

<hr>
        </div>
      </div>
    </div>
  </section>


<section class="section">
<div class="container is-max-desktop">
  <h2 class="title is-3" id="Practical">Part B: A Practical Algorithm - DQC <a href="#Content">↺</a></h2>
  <div class="columns">
    <div class="column">
      <div class="content has-text-justified">
        <p>
          One naïve thing we could do is to simply train an QC agent and then closed-loop execute the learned action chunking policy.
          But this actually does not perform well in practice (as what we will show <a href="#Results">below</a>), likely due to
          the challenge of learning a good action chunking policy with large chunk sizes. 
          <br>
          <br>
          So now the question is—how can we somehow extract a <b>short chunk</b> policy from a <b2>long chunk</b2> 
          critic directly to side-step the challenge of learning action chunking policies with large chunk sizes?
          <br> 
        <hr>
          <br>
          Our solution is simple, just distill a partial chunk size critic from the full chunk size critic, 
          and then extract the policy from this distilled critic! The goal of the distilled critic is to match 
          the value of the full critic given that the second half of the action chunk is optimally picked:
        </p>

        <div class="custom-grey-box">
            <img src="equations/partial-critic.svg" style="width: 100%;" alt="\begin{align}
&Q^P(s_t, a_{t:t+h_a}) \approx Q(s_t, [a_{t:t+h_a}, a^\star_{t+h_a:t+h}]), \text{where } [a_{t:t+h_a}, a^\star_{t+h_a:t+h}]\text{ concatenates two partial action chunks.}
\end{align}"> 
        </div>

        <p>
          To achieve this, we use the expectile loss <a href="https://arxiv.org/abs/2110.06169">(Kostrikov et al., 2022)</a> such that the 
          distilled critic converges to the an upper-expectile of the full critic values, approximating the maximization. The overall value
          learning procedure is summarized below:
        </p>

        <div class="custom-green-box">
            
            <h3>DQC uses a distilled partial critic to extract a policy with shorter action chunks</h3>
            <br>
            <center>
              <img src="equations/agent-update.svg" style="width: 100%;" alt="\begin{align}
  & \text{1. Sample trajectory segment from the dataset }(s_{t:t+h+1}, a_{t:t+h}, r_{t:t+h}) \sim D. \phantom{\sum_{k = 0}^{h-1} \gamma^k r_{t+k} } \\
  & \text{2. {\color[rgb]{0.7929,0.1602,0.4804}[Original full critic: \textbf{long}]} Optimize }Q \text{ with } \left(Q(s_t, a_{t:t+h}) - \sum_{k = 0}^{h-1} \gamma^k r_{t+k} - \gamma^h \bar Q^P(s_{t+h}, \pi(s_{t+h:t+h+h_a})\right)^2. \\
  & \text{3. {\color[rgb]{0,0.46,0.7265}[Distilled partial critic: \textbf{short}]} Optimize }Q^P_\psi \text{ with }  f^{\kappa}_{\text{expectile}}(\bar Q(s_t, a_{t:t+h}) - Q^P(s_t, a_{t:t+h_a})).\phantom{\sum_{k = 0}^{h-1} \gamma^k r_{t+k} } 
  \end{align}"> 
              </center>
        </div>

        <p>
          In summary, DQC trains a policy to predict a <b2>partial chunk</b2> by hill climbing the value of a <b2>partial critic</b2> 
          that is distilled from the <b>original full chunk critic</b> via an implicit maximization loss. 
          This allows our policy to enjoy both the value speedup benefits associated with Q-chunking without explicitly
          predicting the full action chunk. Such design not only <b4>mitigates the learning challenge</b4> of an action chunking 
          policy but also <b4>allows for better reactivity</b4>, enjoying some nice theoretical guarantees as discussed in <a href="#Theory">Part A above</a>. 
          Next, we put our DQC algorithm into test by evaluating it on challenging goal-conditioned RL tasks.
        </p> 
        <hr>
      </div>
    </div>
  </div>
</section>
    

<section class="section">
  <div class="container is-max-desktop ">
    <h2 class="title is-3" id="Results">Results <a href="#Content">↺</a></h2>
    <p>
      We evaluated our method on 6 hardest <a href="https://seohong.me/projects/ogbench/">OGBench</a> 
      environments comparing with a few <b3>ablation baselines</b3>:
    <br>
    <b3>(1) OS</b3>: one-step TD
    <br>
    <b3>(2) NS</b3>: n-step TD
    <br>
    <b3>(3) QC</b3> <a href="https://arxiv.org/abs/2507.07969">(Li et al., 2025)</a>: 
    learning a single chunked critic and then directly 
    extract a policy of the same chunk size from it. Every action chunk is executed open-loop.
    <br>
    <b3>(4) DQC-naïve</b3>: same as above but only execute the partial chunk open-loop. This is a naïve attempt at decoupling the policy and critic chunk sizes without the distilled critic.
    <!-- </ul> -->
    In practice, we also use a separate value function to approximate the Q-target in the n-step TD backup and best-of-N policy extraction similar
    to how it is done in IDQL <a href="https://arxiv.org/abs/2304.10573">(Hansen-Estruch et al., 2023)</a> and SHARSA <a href="https://arxiv.org/abs/2506.04168">(Park et al., 2025)</a>. 
    See more details in <a href="https://arxiv.org/pdf/xxxx.xxxxx">our paper</a>. 
    </p>
    <hr>
    <p>
    <strong>Our method (DQC)</strong> consistently performs on par or better than the baselines across all environments as shown below.
    </p>
    <div class="column">
      <div class="content has-text-justified">
        <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0 auto; margin-bottom: 0px;">
          <img src="release-main.svg" style="width: 100%;" alt="Main result plot">
        </div>
      </div>
    </div>
    <hr>
    <p>
    Our method also outperforms the previous SOTA method, SHARSA <a href="https://arxiv.org/abs/2506.04168">(Park et al., 2025)</a>, across all environments except on cube-octuple where they are similar. 
    The aggregated results for both our ablation baselines and prior works are shown below (10 seeds with 95% CI).
    </p>
    <div class="column">
      <div class="content has-text-justified">
          <div style="display: flex; justify-content: center; gap: 1%; width: 100%; margin: 0 auto; margin-bottom: 0px;">
            <img src="dqc-bar-full.svg" style="width: 100%;" alt="Aggregated bar plot (full)">
          </div>
          <hr>
      </div>
    </div>
    <p>
    That is all for now! We have released our code and all the experiment data for our main results at <a href="https://github.com/colinqiyangli/dqc">github.com/colinqiyangli/dqc</a>. 
    If you are interested in learning more about the paper, come check out <a href="https://arxiv.org/pdf/xxxx.xxxxx">our full paper on arXiv!</a>
    </p>
  </div>
     
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop">
    <h2 class="title">BibTeX</h2>
    <pre>
<code>@article{li2025dqc,
  author = {Qiyang Li and Seohong Park and Sergey Levine},
  title  = {Decoupled Q-chunking},
  conference = {arXiv Pre-print},
  year = {2025},
  url = {http://arxiv.org/abs/xxxx.xxxxx},
}</code></pre></div>
</section>

<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>